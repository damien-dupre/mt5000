---
title: "MT5000 - Data Analytics & Visualization"
subtitle: "Lecture 6: Introduction to Statistics (2)"
author: "Damien Dupré"
date: "Dublin City University"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis", "metropolis-fonts", "css/custom_design.css"]
    lib_dir: libs
    nature:
      beforeInit: "libs/cols_macro.js"
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# general options --------------------------------------------------------------
options(
  scipen = 999,
  htmltools.preserve.raw = FALSE,
  knitr.kable.NA = " - "
  )
set.seed(42)
# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE,
  cache = FALSE,
  comment = "", 
  fig.align = "center", 
  fig.retina = 3
  )
# libraries --------------------------------------------------------------------
library(tidyverse)
library(knitr)
library(kableExtra)
library(nomnoml)
library(DiagrammeR)
library(ggrepel)
library(fontawesome)
library(tweetrmd)
library(plotly)
library(gapminder)
library(patchwork)
library(webshot2)
library(VennDiagram)
library(countdown)
# data -------------------------------------------------------------------------
dnd <- readr::read_csv(here::here("data/dnd.csv"))
```

# Previoulsy ...

We have seen how to:

- Extract data from databases with SQL queries
- Clean and Transform data tables with MS Excel
- Visualise data with interactive Tableau dashboards

```{r out.width='80%'}
include_graphics("https://d33wubrfki0l68.cloudfront.net/571b056757d68e6df81a3e3853f54d3c76ad6efc/32d37/diagrams/data-science.png")
```

**It is time to model the data and to perform statistical analyses!**

---

# Why Statistics?

Scientific reasoning  requires to engage in **induction**, making wise guesses about the relationship between 2 or more variables (also called **Hypothesis**) and to make generalisations about the world.

Every hypothesis has to state relationship between variables also called $H_a$ (for alternative hypothesis) or $H_1$

Every hypothesis has a null hypothesis counterpart (no relationship between variables) also called $H_0$ (pronounce H naught or H zero)
  
**Statistics are used to test the probability of obtaining your results if the Null Hypothesis is true. If this probability is low, then we reject the Null Hypothesis (and consider the Alternative Hypothesis as credible).**

---

# Today's Menu

Statistics is not (only) about producing numbers, it's a method to insure that results obtained about the relationship between two or more variables with a sample can be generalised to the overall population.

Therefore, it is necessary to understand:

1. What are models and how to represent them with an equation?
2. How to test the model's equation and taking a decision about its hypotheses?
3. How to report the results of statistical analyses?

---
class: inverse, mline, center, middle

# 1. Predictor, outcome and control variables

---

# Categories of Variable

Variables can have different types:

- **Categorical**: If the variable's possibilities are words or sentences (character string)
  - if the possibilities cannot be ordered: Categorical Nominal (*e.g.*, gender)
  - if the possibilities can be ordered: Categorical Ordinal (*e.g.*, opinion from disagree to agree)
- **Continuous**: If the variable's possibilities are numbers (*e.g.*, age, temperature) 

> Warning: Variables can be converted to either Categorical and Continuous but it is always better to keep them in their correct scale.

---

# Predictors, Outcomes and Controls

It's important to keep the two roles "variable doing the explaining" and "variable being explained" distinct.

Let's denote the:
 - Outcome: "to be explained" variable as Y (also called Dependent Variable or DV)
 - Predictor: "doing the explaining" as X (also called Independent Variable or IV)

Statistics is only about identifying relationship between variables also called **effect**

> An effect between 2 variables mean that the changes in the values of a predictor variable are related to changes in the values of an outcome variable

---

# Predictors, Outcomes and Controls

An effect between a predictor variable $X$ and an outcome variable $Y$ corresponds to the following model:

```{nomnoml, fig.width=12, fig.height=3}
#stroke: black
#direction: right
#align: center

[X]->[Y]
```

To prove this effect, an hypothesis should be formulated and tested:
- "the variable $Y$ is explained by the variable $X$"
- "the variable $X$ is explaining the variable $Y$"
- "the variable $X$ influences the variable $Y$"
- "there is an effect/ a relationship between $X$ and $Y$"

Note, despite having different formulation these hypotheses are all about the same effect suggesting a correlation which is not causation.

---

# Predictors, Outcomes and Controls

To decide, if the part of the shared variability is big enough to state that effect between the variables can be generalised to the overall population, a statistical test is required.

The statistical test produces a *p-value* which is between 0% and 100% which corresponds to a value between 0.0 and 1.0

**If the *p-value* is lower to 5% or 0.05, then the probability to obtain your results knowing that there is no effect between the variables is low enough to say ...**

.center[**... "There MUST be an effect between these variables!"**]
  
---
class: inverse, mline, center, middle

# 2. How to formulate hypotheses?

---

# Hypotheses in a Nutshell

Hypotheses are:

1. Predictions supported by theory/literature
2. Affirmations designed to precisely describe the relationships between variables 

> *“Hypothesis statements contain two or more variables that are measurable or potentially measurable and that specify how the variables are related”* (Kerlinger, 1986)

Hypotheses include:

- Predictor(s) / Independent Variable(s)
- Outcome / Dependent Variable (DV)
- Direction of the outcome if the predictor increases

But there is only two kind of hypotheses: Main effect hypotheses and Interaction effect hypotheses

---

# Main Effect Hypothesis

Is the predicted relationship between one predictor and one outcome variable

Effect representation:

```{r eval=TRUE, fig.align="left"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    
    'Predictor' -> 'Outcome'
  }", height = 200)
```

Warning: The direction of the arrow does not involve causality, only correlation.

---

# Main Effect Hypothesis

Its formulation depend on the type of predictor: Continuous or Categorical

--

- If the predictor is Continuous:

.center[The {**outcome**} {**increases or decreases**} when {**predictor**} {*increases/decreases/changes*}]

> Example: **Job satisfaction** *increases* when **salary** **increases**

--

- If the predictor is Categorical:

.center[The {**outcome**} of {**predictor's category 1**} is {*higher/lower/different*} than the {**outcome**} of {**predictor's category 2**}]

> Example: **Job satisfaction** of **Irish employees** is *higher* than **job satisfaction** of **French employees**

---

# Interaction Effect Hypothesis

It predicts the influence of a second predictor on the relationship between a first predictor and an outcome variable

Notes:

- The second predictor is also called moderator.
- The role of first and second predictors can be inverted with the exact same statistical results

Effects representation:

```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node []
    'Predictor 1'; 'Predictor 2'; Outcome
    node [shape = point, width = 0, height = 0]
    ''
    
    'Predictor 2' -> ''
    'Predictor 1' -> '' [arrowhead = none]
    ''-> Outcome
    
    subgraph {
      rank = same; 'Predictor 2'; '';
    }
  }", height = 200)
```

---

# Interaction Effect Hypothesis

The easiest to formulate an interaction effect hypotheses is to imagine the second predictor has two categories even if the it is continuous (e.g., age can be converted to younger vs. older)

Note: this conversion is to help the formulation of the hypothesis but the analysis has to be done with the actual continuous numbers

--

Formulation structure:

.center[The effect of {**predictor 1**} on {**outcome**} is {*higher/lower/different*} for {**predictor's category 1**} than for {**predictor's category 2**}]

Examples: 

> The effect of **Salary** on **Job Satisfaction** is *higher* for **Irish employees** than for **French employees**

> The effect of **Salary** on **Job Satisfaction** is *higher* for **younger employees** than for **older employees**

---
class: inverse, mline, center, middle

# 3. Representing your Model

---

# Structure of Models

Are graphic representations of all variables investigated and their predicted relationships.

- All the relations correspond to an hypothesis to be tested
- All the tested hypotheses have to be in the model

Generally, models are built from scratch to match with the formulated hypotheses

However, if the hypotheses are taken from previous academic research, a model can be copy-pasted as well by justifying its interest for the research

---

# Structure of Models

A model is the representation of one ore more effects

.pull-left[
In a model, **a simple arrow is a main effect**

Example:

```{r eval=TRUE}
DiagrammeR::grViz("
digraph rmarkdown {
  graph [rankdir = LR]
  
  node [shape = oval]
  'Salary'; 'Job Satisfaction'
        
  'Salary' -> 'Job Satisfaction'
}
", width = 400, height = 100)
```

It can be read as "Salary has an effect on Job Satisfaction".
]

.pull-right[

In a model, **an arrow crossing another arrow is an interaction effect**

Example:

```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node []
    'Salary'; 'Job Satisfaction'; 'Experience'
    node [shape = point, width = 0, height = 0]
    ''
    
    'Experience' -> ''
    'Salary' -> '' [arrowhead = none]
    ''-> 'Job Satisfaction'
    
    subgraph {
      rank = same; 'Experience'; '';
    }
  }", width = 400, height = 100)
```

It can be read as:
- "Salary has a main effect on Job Satisfaction".
- "Usefulness has a main effect on Job Satisfaction".
- "Salary and Experience have an interaction effect on Job Satisfaction".

]

---

# Main Effect Relationship

.pull-left[
The relation between one variable and another:

```{r fig.align="left"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    
    X -> Y
  }", height = 200, width = 200)
```

can be read as:
- "X has a **main** effect on Y"
]
.pull-right[
A model can have multiple main effects: 

```{r eval=TRUE}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    
    X -> Y
    Z -> Y
  }", height = 200, width = 200)
```

can be read as: 
- "X has a **main** effect on Y"
- "Z has a **main** effect on Y"
]

---

# Interaction Effect Relationship

The relation between two predictor variables and another outcome variable. An interaction means that **the effect of X on Y will be different according the possibilities of Z** (also called Moderation).

.pull-left[
classic representation:
```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = circle]
    X; Y; Z
    node [shape = point, width = 0, height = 0]
    ''
    
    Z -> ''
    X -> '' [arrowhead = none]
    ''-> Y
    
    subgraph {
      rank = same; Z; '';
    }
  }", height = 200, width = 300)
```
]

.pull-right[
is the same as:
```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    
    X -> Y
    Z -> Y
    'X*Z' -> Y
  }", height = 200, width = 200)
```
]

The arrow crossing the main effect arrow can be read as :
- "X have a **main** effect on Y"
- "Z have a **main** effect on Y"
- "X and Z have an **interaction** effect on Y"

---

# Types of Model

## Simple Model

- One or more predictors
- Only one outcome
- Made of main or/and interaction effects

## Mediation Model (simple or moderated)

- At least 2 predictors (one call Mediator)
- Only one outcome
- Made of main effects only for simple mediation / main and interaction effects for moderated mediation

## Structural Equation Model (SEM)

- At least 2 predictors (usually latent variables)
- One or more outcome
- Made of main or/and interaction effects

---

# Simple Model

Simple Models are the most statistically powerful, easy to test and reliable models. Always prefer a simple model compared to a more complicated solution.

Warning, including interaction effect requires a significantly higher sample size (see calculation of power/effect size).

Example:

```{r eval=TRUE}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = circle]
    'Salary'; 'Job Satisfaction'; 'Gender'; 'Age'
    node [shape = point, width = 0, height = 0]
    ''
    
    'Age' -> 'Job Satisfaction'
    'Gender' -> ''
    'Salary' -> '' [arrowhead = none]
    '' -> 'Job Satisfaction'
    
    subgraph {
      rank = same; 'Gender'; '';
    }
    subgraph {
      rank = same; 'Age'; 'Salary';
    }
  }", height = 300, width = 800)
```

---

# Mediation Models

A Mediation model is a complex path analysis between 3 variables, where one of them explains the relationship between the other two. It is usually used to identify cognitive process in psychology.

Example:

```{r eval=TRUE, fig.align="left"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node []
    'self-esteem'; 'math exam results'; happiness
    
    'math exam results' -> {happiness 'self-esteem'}
    'self-esteem' -> happiness

  }", height = 300, width = 800)
```

---

# Structural Equation Model

A Structural Equation Model (SEM) is a complex path analysis between multiple variables including multiple Outcomes and using factor analysis for latent variable estimation.

```{r eval=TRUE}
DiagrammeR::grViz("
digraph rmarkdown {
  graph [rankdir = LR]
  
  node [shape = oval]
  'Perceived Ease-of-use'; 'Perceived Usefulness'; 'Intention to Use'; 'Actual Use'
  
  node [shape = box]
  PU1; PU2; PU3; PU4; PU5; PEOU1; PEOU2; PEOU3; PEOU4; BI1; BI2; AU
  
  {PU1 PU2 PU3 PU4 PU5} -> 'Perceived Usefulness' [arrowhead = none]
  {PEOU1 PEOU2 PEOU3 PEOU4} -> 'Perceived Ease-of-use' [arrowhead = none]
  {BI1 BI2} -> 'Intention to Use' [arrowhead = none]
  {AU} -> 'Actual Use' [arrowhead = none]
  
  'Perceived Usefulness' -> 'Intention to Use'
  'Perceived Ease-of-use' -> {'Perceived Usefulness' 'Intention to Use'}
  'Intention to Use' -> 'Actual Use'
  
  subgraph {
      rank = same; 'Perceived Usefulness'; 'Perceived Ease-of-use';
  }
  
  subgraph {
      rank = same; PU1; PU2; PU3; PU4; PU5; PEOU1; PEOU2; PEOU3; PEOU4; BI1; BI2;
  }

}
", height = 500, width = 800)
```

---

# A Good Model

- Comprehensiveness: Explains a wide range of phenomena
- Internal Consistency: Propositions and assumptions are consistent and fit together in a coherent manner
- Parsimony: Contains only those concepts and assumptions essential for the explanation of a phenomenon
- Testability: Concepts and relational statements are precise.
- Empirical Validity: Holds up when tested in the real world.

Example: 

```{r eval=TRUE}
DiagrammeR::grViz("
digraph rmarkdown {
  graph [rankdir = LR]
  
  node [shape = oval]
  'Perceived Ease-of-use'; 'Perceived Usefulness'; 'Intention to Use'
  
  node [shape = box]
  'Actual Use'
  
  'Perceived Usefulness' -> 'Intention to Use'
  'Perceived Ease-of-use' -> {'Perceived Usefulness' 'Intention to Use'}
  'Intention to Use' -> 'Actual Use'
  
  subgraph {
      rank = same; 'Perceived Usefulness'; 'Perceived Ease-of-use';
  }
}
", height = 300, width = 800)
```

---

# A Bad Theory/Model

- too complicated
- does not explain many things
- cannot be tested

Is it bad?

![](https://d3i71xaburhd42.cloudfront.net/2a3e4a3024bfc4df27db07a1d48f77a6f371b0c3/8-Figure1-1.png)

---
class: inverse, mline, center, middle

# 4. Understanding the Equation

---

# What is a Correlation?

A correlation indicate the strength and direction of a relationship (slope):
- Uses the letter $r$ without statistical tests
- Uses the letter $\beta$ with statistical tests

It has a value between 1 (strong & positive relationship) and -1 (strong and negative relationship) where 0 indicates no relationship

For each hypothesis, the statistical test will check if the $\beta$ correlation corresponding to the predicted effects under the $H_a$s are significantly different than 0 as expected under $H_0$s

.pull-left[
.center[Model with 1 main effect]
```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    
    Predictor -> Outcome [label= β1]
  }", height = 200, width = 200)
```
]

.pull-right[
.center[Model with 2 main and 1 interaction effects]
```{r}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = circle]
    'Predictor 1'; Outcome; 'Predictor 2'
    node [shape = point, width = 0, height = 0]
    ''
    
    'Predictor 2' -> '' [label= β2]
    'Predictor 1' -> '' [arrowhead = none] [label= β1]
    ''-> Outcome [label= β3]
    
    subgraph {
      rank = same; 'Predictor 2'; '';
    }
  }", height = 200, width = 300)
```
]

---

# What is a Correlation?

```{r out.width='100%'}
knitr::include_graphics("img/correlation.png")
```

---

# Evaluation of the Significance

Once all the hypotheses are formulated, it is time to test all of them in one unique model. In this model, a value called $\beta$ which is usually the coefficient of correlation is associated to each predictor from the hypotheses. 

Testing for the significance of the effect means evaluating if this $\beta$ value is significantly different, higher or lower than 0 (no link between variables):

- $H_a: \beta \neq 0$ means our hypothesis doesn't precise the direction of the change, just that there is a change

- $H_a: \beta > 0$ means our hypothesis indicates that the relationship increases or a group is higher than another group

- $H_a: \beta < 0$ means our hypothesis indicates that the relationship decreases or a group is lower than another group

- The Null hypothesis will always be $H_0: \beta = 0$

---

# A Basic Equation

The basic structure of a statistical model is:

$$Outcome = Model + Error$$

where the $Model$ is a series of predictors that are expressed in hypotheses.

This expresses the idea that the Outcome can be described by a statistical model, which expresses what we expect to occur in the Outcome, along with the difference between the model and the Outcome, which we refer to as the error.

---

# A Basic Equation

```{r}
df <- 
  data.frame(
    Observation = letters[1:11],
    Outcome = 10:0, 
    Predictor = 10:0
  ) 
```


Let's imagine the perfect scenario: **your predictor Predictor variable explains perfectly the outcome variable**.

The corresponding equation is: $Outcome = Predictor$

.pull-left[
```{r}
df %>% 
  kable(align = "ccc") %>%
  kable_styling(bootstrap_options = "striped", font_size = 14)
```
]
.pull-right[
```{r fig.height=7}
df %>% 
  ggplot(aes(Predictor, Outcome, label = Observation)) +
  geom_point(color = "black", size = 5) +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0,10)) +
  scale_y_continuous(limits = c(0,10)) +
  theme_bw() +
  theme(text = element_text(size = 20))
```
]

---

# A Basic Equation

In the equation $Outcome = Predictor$, three coefficients are hidden because they are unused:
- the **intercept coefficient** $\beta_{0}$ (i.e., the value of the Outcome when the Predictor = 0) which is 0 in our case
- the **slope coefficient** $\beta_{1}$  (i.e., how much the Outcome increases when the Predictor increases by 1) which is 1 in our case
- the **error coefficient** $\epsilon$ (i.e., how far from the prediction line the values of the Outcome are) which is 0 in our case

So in general, the relation between a predictor and an outcome can be written as:
$$Outcome = \beta_{0} + \beta_{1} * Predictor + \epsilon$$

which is in our case:

$$Outcome = 0 + 1 * Predictor + 0$$

---

# A Basic Equation

The equation $Outcome = \beta_{0} + \beta_{1} * Predictor + \epsilon$ is the same as the good old $y = ax + b$ (here ordered as $y = b + ax$) where $\beta_{0}$ is $b$ and $\beta_{1}$ is $a$.

It is very important to know that under **EVERY** statistical test, a similar equation is used (t-test, ANOVA, Chi-square are all linear regressions).

```{r fig.width=5.5, fig.height=5.5, fig.align='center'}
plot0 <- data.frame(Predictor = 0:10, Outcome = 0:10) %>%
  ggplot(aes(Predictor, Outcome)) +
  geom_point(color = "black", size = 5) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 0, color = 'black', size = 0.5, linetype = 'dotted') +
  annotate("text", x = 5, y = 0.2, label = "Intercept \u03b2\u2080") +
  annotate('segment', x = 5, xend = 6, y = 5, yend = 5, color = 'red') +
  annotate('segment', x = 6, xend = 6, y = 5, yend = 6, color = 'red') +
  annotate("text", x = 7.5, y = 5.5, label = "Slope \u03b2\u2081") +
  scale_x_continuous(breaks = seq(0:10)) +
  scale_y_continuous(breaks = seq(0:10)) +
  theme_bw()

plotly::ggplotly(plot0)
```

---

# Equations, Variables and Effect Types

Let's come back to our example, involving employee's job satisfaction

```{r}
dnd %>% 
  select(employee, location, salary, js_score) %>% 
  kable(digits = 2)
```

---

# Equations, Variables and Effect Types

Except in special cases:
- An Outcome (or Dependent Variable) has to be Continuous
- A Predictor can be Continuous or Categorical 

Example: $Job\,Satisfaction = \beta_{0} + \beta_{1}.Salary + \beta_{2}.Location + \epsilon$

In this equation:
- $Salary$ is continuous with a main effect on $Job\,Satisfaction$ ( $\beta_{1}$)
- $Location$ is categorical with a main effect on $Job\,Satisfaction$ ( $\beta_{2}$) 

An interaction effect is represented by multiplying the 2 predictors involved:

$$Job\,Satisfaction = \beta_{0} + \beta_{1}.Salary + \beta_{2}.Location + \beta_{3}.Salary*Location + \epsilon$$

In this equation:
- $Salary$ is continuous with a main effect on $Job\,Satisfaction$ ( $\beta_{1}$)
- $Location$ is categorical with a main effect on $Job\,Satisfaction$ ( $\beta_{2}$)
- $Salary$ and $Location$ have an interaction effect on $Job\,Satisfaction$ ( $\beta_{3}$)

---

# Relevance of the Intercept

To test hypotheses, only the $\beta$ values associated to Predictors / Independent Variables are important.

The intercept is always included in an equation but its result is useless for hypothesis testing.

Let's see why the intercept is always included but its results discarded.

Imagine we want to test the relationship between Salary and Job Satisfaction. Let's compare a model without and a model with intercept:

- Without intercept: $Job\,Satisfaction = \beta_{1}.Salary + \epsilon$

- With intercept: $Job\,Satisfaction = \beta_{0} + \beta_{1}.Salary + \epsilon$

---

# Relevance of the Intercept

```{r}
p1 <- dnd %>% 
  ggplot(aes(x = salary, y = js_score)) +
  geom_point() +
  scale_y_continuous(limits = c(0, 10)) +
  labs(
    title = "A: Original Data",
    x = "Salary",
    y = "Job Satisfaction"
  )

p2 <- dnd %>% 
  ggplot(aes(x = salary, y = js_score)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, fullrange = TRUE, formula = "y ~ x + 0 ") +
  scale_y_continuous(limits = c(0, 10)) +
  labs(
    title = "B: Model without intercept",
    x = "Salary",
    y = "Job Satisfaction"
  )

p3 <- dnd %>% 
  ggplot(aes(x = salary, y = js_score)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, fullrange = TRUE, formula = "y ~ x") +
  scale_y_continuous(limits = c(0, 10)) +
  labs(
    title = "C: Model with intercept",
    x = "Salary",
    y = "Job Satisfaction"
  )

p1 / (p2 + p3)
```

If the intercept is not included, the intercept is zero and can lead to estimation errors

---
class: title-slide, middle

## Exercise: Predicates of Statistical Analyses

Find variables, formulate hypotheses, draw model, and write equation corresponding to the following data

---

# Data 1

Find variables, formulate hypotheses, draw model, and write equation corresponding to the following data

.pull-left[

.center[Data]

```{r}
  data.frame(
    participant = c("ppt1", "ppt2", "ppt3", "ppt4", "ppt5", "ppt6"), 
    IQ = c(120, 103, 96, 87, 114, 95),
    exam_mark = c(89, 64, 71, 77, 78, 69)
  ) %>% 
  kable()
```
]
.pull-right[

.center[Visualisation]

```{r fig.height=6}
  data.frame(
    participant = c("ppt1", "ppt2", "ppt3", "ppt4", "ppt5", "ppt6"), 
    IQ = c(120, 103, 96, 87, 114, 95),
    exam_mark = c(89, 64, 71, 77, 78, 69)
  ) %>% 
  ggplot(aes(x = IQ, y = exam_mark, label = participant)) +
  geom_point(color = "black", size = 5) +
  geom_text_repel(size = 10) +
  theme_bw() +
  theme(
    text = element_text(size = 20)
  )
```
]

```{r}
countdown(minutes = 5, warn_when = 60, right =  1)
```

---

# Data 1

Variables:
- Outcome = exam_mark (from 0 to 100)
- Predictor = IQ (from 0 to Inf.)

Hypothesis:
- $H_a$: The **exam_mark** *increases* when **IQ** increases
- $H_0$: The **exam_mark** *stay the same* when **IQ** increases

Model:
```{r eval=TRUE, fig.align="center"}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
    node [shape = circle]
    
    IQ -> exam_mark [label= β1]
  }", height = 100, width = 500)
```

Equation:
- $exam\_mark = \beta_{0} + \beta_{1}.IQ + \epsilon$

---

# Data 2

.pull-left[

.center[Data]

```{r, eval=TRUE}
df4 <- data.frame(
  participant = c("ppt1", "ppt2", "ppt3", "ppt4", "ppt5", "ppt6"), 
  IQ = c(120, 103, 96, 87, 114, 95),
  age = c(50, 60, 70, 20, 30, 40),
  exam_mark = c(89, 64, 71, 77, 52, 69),
  age_c = c("experienced", "experienced", "experienced", "beginner", "beginner", "beginner")
)

df4 %>% 
  select(-age_c) %>% 
  kable()
```
]
.pull-right[

.center[Visualisation]

```{r fig.height=6}
df4 %>% 
  ggplot(aes(x = IQ, y = exam_mark, color = age_c)) +
  geom_point(size = 5) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_bw() +
  theme(
    text = element_text(size = 14)
  )
```
]

```{r}
countdown(minutes = 5, warn_when = 60, right =  1)
```

---

# Data 2

Variables:
- Outcome = exam_mark (from 0 to 100)
- Predictor 1 = IQ (from 0 to Inf.)
- Predictor 2 = age (from 0 to Inf. but categorised for visualisation)

Hypotheses: 
- $H_{a_{1}}$: The **exam_mark** *increases* when **IQ** increases
- $H_{0_{1}}$: The **exam_mark** *stay the same* when **IQ** increases

- $H_{a_{2}}$: The **exam_mark** *increases* when **age** increases
- $H_{0_{2}}$: The **exam_mark** *stay the same* when **age** increases

- $H_{a_{3}}$: The effect of **IQ** on **exam_mark** is *higher* for **beginner** than for **experienced students**
- $H_{0_{3}}$: The effect of **IQ** on **exam_mark** is *the same* for **beginner** than for **experienced students**

---

# Data 2

Model:

.pull-left[

.center[Classic Representation]
```{r eval=TRUE}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = circle]
    IQ; exam_mark; age
    node [shape = point, width = 0, height = 0]
    ''
    
    age -> ''
    IQ -> '' [arrowhead = none]
    ''-> exam_mark
    
    subgraph {
      rank = same; age; '';
    }
  }", height = 300, width = 400)
```
]

.pull-right[

.center[Effects Correspondance]
```{r eval=TRUE}
DiagrammeR::grViz("
  digraph {
    graph [rankdir = LR]
  
    node [shape = circle]
    IQ; exam_mark;'IQ*age'; age

    age -> exam_mark [label= β2]
    IQ -> exam_mark [label= β1]
    'IQ*age' -> exam_mark [label= β3]
    
  }", height = 300, width = 300)
```
]

Equations:

- $exam\_mark = \beta_{0} + \beta_{1}.IQ + \beta_{2}.age + \beta_{3}.interaction + \epsilon$

which corresponds to: $exam\_mark = \beta_{0} + \beta_{1}.IQ + \beta_{2}.age + \beta_{3}.IQ*age + \epsilon$ 

---
class: inverse, mline, left, middle

<img class="circle" src="https://github.com/damien-dupre.png" width="250px"/>

# Thanks for your attention and don't hesitate if you have any question!

[`r fontawesome::fa(name = "twitter")` @damien_dupre](http://twitter.com/damien_dupre)  
[`r fontawesome::fa(name = "github")` @damien-dupre](http://github.com/damien-dupre)  
[`r fontawesome::fa(name = "link")` damien-datasci-blog.netlify.app](https://damien-datasci-blog.netlify.app)  
[`r fontawesome::fa(name = "paper-plane")` damien.dupre@dcu.ie](mailto:damien.dupre@dcu.ie)